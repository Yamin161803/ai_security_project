{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project\n",
    "\n",
    "Philip WollsÃ©n Ervius \\\n",
    "phao21@student.bth.se\n",
    "\n",
    "Amin Afzali \\\n",
    "moaf@student.bth.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import CrossEntropyLoss, Linear\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "The CIFAR-10 dataset will be transformed to fit the ImageNet-1k data it was pretrained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for CIFAR-10 (resize and normalize to match ImageNet-1K pretraining)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    # transforms.Resize(256),       # Resize to 256px on the shorter side\n",
    "    # transforms.CenterCrop(224),   # Center crop to 224x224\n",
    "    transforms.ToTensor(),        # Convert to tensor\n",
    "    # transforms.Normalize(\n",
    "        # mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "        # std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "    # ),\n",
    "])\n",
    "\n",
    "\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),        # Convert to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR-10 dataset\n",
    "\n",
    "We will split the dataset into training and test partitions. Then, we get a subset from the test partition to use for attacks, since they will be new to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=preprocess, download=True)\n",
    "full_test_dataset = CIFAR10(root=\"./data\", train=False, transform=preprocess, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "nr_of_attack_images = 1000\n",
    "\n",
    "test_dataset = Subset(full_test_dataset, range(nr_of_attack_images, len(full_test_dataset)))\n",
    "attack_dataset = Subset(full_test_dataset, range(nr_of_attack_images))\n",
    "\n",
    "attack_loader = DataLoader(attack_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dict with the names for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained model\n",
    "\n",
    "We will be using a pretrained model gotten from the public repository https://github.com/chenyaofo/pytorch-cifar-models?tab=readme-ov-file. It is a fine-tuned Moblienet-V2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model):\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predicted)\n",
    "\n",
    "    # Concatenate all tensors\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_predictions = torch.cat(all_predictions).numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_images(images: list, classify= None, extra_info= None, fig_title = None):\n",
    "    \"\"\"Displays a list of images.\n",
    "\n",
    "    Parameters:\n",
    "        - images: list of torch images.\n",
    "        - classify: Optional bool whether to classify images using model.\n",
    "        - extra_info: Optional dict[int, str] or list[str] containing extra information\n",
    "                                to be displayed for certain images.\n",
    "        - fig_title: Optional figure title.\n",
    "    \"\"\"\n",
    "\n",
    "    if not images:\n",
    "        return\n",
    "\n",
    "    max_cols = 5\n",
    "\n",
    "    if len(images) >= max_cols:\n",
    "        cols = max_cols\n",
    "    else:\n",
    "        cols = len(images) % max_cols\n",
    "\n",
    "    rows = len(images) // cols\n",
    "\n",
    "    if rows * cols < len(images):\n",
    "        rows += 1\n",
    "\n",
    "    width = 3.8*cols\n",
    "    height = 3.2*rows\n",
    "    if fig_title:\n",
    "        height += 2.8\n",
    "\n",
    "    if cols <= 2:\n",
    "        width *= 1.7\n",
    "        height *= 1.7\n",
    "    elif cols == 3:\n",
    "        width *= 1.5\n",
    "        height *= 1.5\n",
    "    fig = plt.figure(figsize= (width, height))\n",
    "    axes = fig.subplots(nrows= rows, ncols= cols)\n",
    "\n",
    "    if rows == 1 and cols == 1:                 # Make sure axes is always 2D for indicing\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1 or cols == 1:\n",
    "        axes = np.array(axes).reshape(rows, cols)\n",
    "    else:\n",
    "        axes = np.array(axes)\n",
    "\n",
    "    to_pil_image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "\n",
    "        if i >= len(images):\n",
    "            fig.delaxes(axes[row][col])\n",
    "            continue\n",
    "\n",
    "        img = images[i]\n",
    "\n",
    "        if len(img.shape) == 4:\n",
    "            disp_img = to_pil_image(img.squeeze(0).permute(1, 2, 0))\n",
    "        else:\n",
    "            disp_img = to_pil_image(img)\n",
    "\n",
    "        axes[row][col].imshow(disp_img)\n",
    "\n",
    "        title = \"\"\n",
    "\n",
    "        if classify:\n",
    "            with torch.no_grad():\n",
    "                output = model(to_tensor(img).unsqueeze(0))\n",
    "\n",
    "            _, pred = torch.max(output, 1)\n",
    "            title = f\"Predicted: {class_names[pred.item()]}\"\n",
    "        if extra_info:\n",
    "            try:\n",
    "                title += f\"{extra_info[i]}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        axes[row][col].set_title(title)\n",
    "        axes[row][col].axis('off')\n",
    "\n",
    "    fig.patch.set_facecolor(\"lightgray\")\n",
    "\n",
    "    if fig_title:\n",
    "        fig.suptitle(fig_title, fontsize= 28)\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(img, label):\n",
    "    \"\"\"Trains the model on an image.\n",
    "\n",
    "    Parameters:\n",
    "        - img: tensor of preprocessed image.\n",
    "        - label: int target label for the image.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(img.shape) == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Prepare the input and label\n",
    "    labels = torch.tensor([label])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(img)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # print(f'Loss: {running_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack #1: Backdoor trigger\n",
    "\n",
    "We will pick a set of images and add a couple trigger pixels. The goal is for the model to associate the pixels with a certain class, allowing us to achieve the desired output for any image with the trigger in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_poisoned_pixels = 1\n",
    "indices = np.random.randint(0, 32, [nr_of_poisoned_pixels, 2])\n",
    "\n",
    "poison_strength = 10     # Pixel values\n",
    "target_label = 0        # Airplane\n",
    "channel = 1             # Green\n",
    "\n",
    "class PoisonTransform:\n",
    "    def __init__(self, indices, value, channel):\n",
    "        self._indices = indices\n",
    "        self._poison_value = value\n",
    "        self._channel = channel\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for (x, y) in self._indices:\n",
    "            image[self._channel, x, y] = self._poison_value\n",
    "        return image\n",
    "\n",
    "backdoor_dataset = [(PoisonTransform(indices, poison_strength, channel)(image), target_label) for image, label in attack_dataset if label != target_label]\n",
    "\n",
    "attack_samples = []\n",
    "attack_labels = []\n",
    "\n",
    "nr_of_attack_images = min(len(backdoor_dataset), 100)    # For training the model\n",
    "\n",
    "for (image, label), i in zip(backdoor_dataset, range(nr_of_attack_images)):\n",
    "    attack_samples.append(image.permute(1, 2, 0).numpy())\n",
    "    attack_labels.append(label)\n",
    "\n",
    "nr_of_examples = 10\n",
    "display_images(attack_samples[:nr_of_examples], extra_info= [f\"Original class: {class_names[label]}\" for _, label in attack_dataset if label != target_label], fig_title= \"Poisoned images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the poisoned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (image, _), label in zip(backdoor_dataset, attack_labels):\n",
    "    train(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate attack efficiency and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attack_test_samples = []\n",
    "for (image, label), i in zip(backdoor_dataset, range(nr_of_attack_images, len(backdoor_dataset))):\n",
    "    attack_test_samples.append(image.permute(1, 2, 0).numpy())\n",
    "\n",
    "    if i > 1000:\n",
    "        break\n",
    "\n",
    "success = 0\n",
    "with torch.no_grad():\n",
    "    for image in attack_test_samples:\n",
    "\n",
    "        output = model(to_tensor(image).unsqueeze(0))\n",
    "\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "        if pred.item() == target_label:\n",
    "            success += 1\n",
    "\n",
    "success_rate = success / len(attack_test_samples)\n",
    "print(f\"Backdoor attack success rate: {success_rate}\")\n",
    "\n",
    "display_images(attack_test_samples[nr_of_attack_images:nr_of_attack_images+10], classify= True)\n",
    "\n",
    "print(\"\\nModel evaluation after attack:\")\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
